{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3239840f"
      },
      "source": [
        "\n",
        "# ETL Pipeline Explanation\n",
        "\n",
        "This notebook explains the ETL pipeline step by step.\n",
        "\n",
        "## 1. Overall Flow\n",
        "The pipeline:\n",
        "1. **Extracts** data from multiple sources: a CSV file, an API, and a PostgreSQL database.\n",
        "2. **Transforms** the data through cleaning, enrichment, and validation.\n",
        "3. **Loads** the transformed data into a CSV file for storage or further analysis.\n",
        "4. **Logs** each step of the process for monitoring.\n",
        "\n",
        "## 2. Step-by-Step Explanation\n",
        "\n",
        "### Extraction\n",
        "The `extract_data` function gathers data from:\n",
        "1. **CSV File**:\n",
        "   - Attempts to read a local `data.csv` file.\n",
        "   - If the file isn’t found, it logs the event and moves on.\n",
        "\n",
        "2. **API**:\n",
        "   - Calls a placeholder API (`https://jsonplaceholder.typicode.com/posts`).\n",
        "   - If successful, converts the API response (JSON) into a pandas DataFrame.\n",
        "   - Logs the success or failure of the request.\n",
        "\n",
        "3. **PostgreSQL Database**:\n",
        "   - Connects to the PostgreSQL database using provided credentials via `SQLAlchemy`.\n",
        "   - Runs a SQL query (`SELECT ... FROM rnc_database LIMIT 100`) to fetch data.\n",
        "   - Logs whether the connection and data extraction were successful.\n",
        "\n",
        "Finally, it **combines all data** into a single DataFrame using `pd.concat`.\n",
        "\n",
        "### Transformation\n",
        "The `transform_data` function performs several tasks:\n",
        "1. **Cleaning**:\n",
        "   - Drops duplicate rows.\n",
        "   - Removes rows where the `id` column has `NaN` values (as `id` is critical for merging and analysis).\n",
        "\n",
        "2. **Handling List-Like Values**:\n",
        "   - Ensures that the `id` column doesn’t contain unhashable types like lists.\n",
        "\n",
        "3. **Standardization**:\n",
        "   - Converts the `timestamp` column to datetime format, handling errors gracefully.\n",
        "   - Standardizes the `name` column by filling missing values and converting to strings.\n",
        "\n",
        "4. **Feature Engineering**:\n",
        "   - Adds a `name_length` column that computes the length of each `name`.\n",
        "   - Flags records as recent (`is_recent`) if their `timestamp` is within the last 30 days.\n",
        "\n",
        "5. **Data Integration**:\n",
        "   - Merges the main DataFrame with a small reference dataset (`reference_data`) on the `id` column.\n",
        "\n",
        "6. **Validation**:\n",
        "   - Checks for rows with missing `id` values after processing and raises an error if found.\n",
        "\n",
        "### Loading\n",
        "The `load_data` function saves the transformed DataFrame into a CSV file (`transformed_data.csv`).\n",
        "\n",
        "### Logging\n",
        "The `log_pipeline_step` function adds monitoring by logging messages for each step.\n",
        "\n",
        "### Error Handling\n",
        "- **During Extraction**: Skips missing sources without breaking the pipeline.\n",
        "- **During Transformation**: Handles invalid or missing values explicitly.\n",
        "- **General**: Logs unhandled errors and tracks the failed step.\n",
        "\n",
        "## 3. Final Flow\n",
        "1. **Start the Pipeline**: Initializes and logs the start.\n",
        "2. **Extract**: Gathers data from CSV, API, and database sources.\n",
        "3. **Transform**: Cleans, enriches, standardizes, and integrates data.\n",
        "4. **Load**: Saves the transformed data into a CSV file.\n",
        "5. **Complete**: Logs the success or failure of the pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you need clarification or enhancements!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "r2jBwswnG6xZ",
        "outputId": "b44eeb7b-095b-4848-82f6-447e0a3f18e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b5c5bf65-8851-4dfa-8085-eec20fca880b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b5c5bf65-8851-4dfa-8085-eec20fca880b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.csv to data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQi2sXnlHBlP",
        "outputId": "9c332b60-e179-41e8-ebaf-6c3c98ad4f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'data.csv', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSTZwkEx8pvK",
        "outputId": "a05e8a6d-bd0f-44f4-d7b4-18f3c03262e9"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# PostgreSQL connection details\n",
        "host = \"hh-pgsql-public.ebi.ac.uk\"\n",
        "port = 5432\n",
        "database = \"pfmegrnargs\"\n",
        "user = \"reader\"\n",
        "password = \"NWDMCE5xdipIjRrp\"\n",
        "\n",
        "# Step 1: Extraction\n",
        "def extract_data():\n",
        "    \"\"\"\n",
        "    Extracts data from multiple sources, including a CSV file, an API, and a PostgreSQL database.\n",
        "    \"\"\"\n",
        "    # Example 1: Load data from a CSV file\n",
        "    try:\n",
        "        csv_data = pd.read_csv(\"data.csv\")\n",
        "        log_pipeline_step(\"extraction\", \"CSV data extracted successfully\")\n",
        "    except FileNotFoundError:\n",
        "        log_pipeline_step(\"extraction\", \"No CSV file found, skipping this source\")\n",
        "        csv_data = pd.DataFrame()\n",
        "\n",
        "    # Example 2: Extract data from an API\n",
        "    api_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
        "    response = requests.get(api_url)\n",
        "    if response.status_code == 200:\n",
        "        api_data = pd.DataFrame(response.json())\n",
        "        log_pipeline_step(\"extraction\", \"API data extracted successfully\")\n",
        "    else:\n",
        "        log_pipeline_step(\"extraction\", f\"Failed to fetch API data: {response.status_code}\")\n",
        "        api_data = pd.DataFrame()\n",
        "\n",
        "    # Example 3: Extract data from PostgreSQL\n",
        "    try:\n",
        "        connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
        "        engine = create_engine(connection_string)\n",
        "        query = \"\"\"\n",
        "                    SELECT id, description, avg_length, min_length, max_length, num_sequences, num_organisms\n",
        "                    FROM rnc_database\n",
        "                    LIMIT 100;\n",
        "                \"\"\"\n",
        "        db_data = pd.read_sql(query, con=engine)\n",
        "        log_pipeline_step(\"extraction\", \"Database data extracted successfully\")\n",
        "    except Exception as e:\n",
        "        log_pipeline_step(\"extraction\", f\"Failed to connect to PostgreSQL: {e}\")\n",
        "        db_data = pd.DataFrame()\n",
        "\n",
        "    # Combine extracted data\n",
        "    extracted_data = pd.concat([csv_data, api_data, db_data], ignore_index=True)\n",
        "    return extracted_data\n",
        "\n",
        "# Step 2: Transformation\n",
        "def transform_data(data):\n",
        "    \"\"\"\n",
        "    Perform comprehensive data cleaning, standardization, enrichment, and validation.\n",
        "    \"\"\"\n",
        "    # Data Cleaning\n",
        "    data.drop_duplicates(inplace=True)\n",
        "    data.dropna(subset=[\"id\"], inplace=True)\n",
        "\n",
        "    # Ensure `id` and other key columns are not lists or unhashable\n",
        "    if \"id\" in data.columns:\n",
        "        data = data[data[\"id\"].apply(lambda x: not isinstance(x, list))]\n",
        "\n",
        "    # Standardization\n",
        "    if \"timestamp\" in data.columns:\n",
        "        data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "    if \"name\" in data.columns:\n",
        "        # Convert all `name` values to strings, fill NaN with empty strings\n",
        "        data[\"name\"] = data[\"name\"].fillna(\"\").astype(str)\n",
        "        data[\"name_length\"] = data[\"name\"].apply(len)\n",
        "\n",
        "    # Add a flag for recent records (example logic)\n",
        "    if \"timestamp\" in data.columns:\n",
        "        recent_date = pd.Timestamp.now() - pd.Timedelta(days=30)\n",
        "        data[\"is_recent\"] = data[\"timestamp\"] > recent_date\n",
        "\n",
        "    # Data Integration: Simulate joining with another dataset\n",
        "    reference_data = pd.DataFrame({\n",
        "        \"id\": [1, 2, 3, 4],\n",
        "        \"region\": [\"North\", \"South\", \"East\", \"West\"]\n",
        "    })\n",
        "\n",
        "    # Ensure `id` is valid and merge\n",
        "    if \"id\" in data.columns:\n",
        "        data[\"id\"] = pd.to_numeric(data[\"id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    try:\n",
        "        data = data.merge(reference_data, on=\"id\", how=\"left\")\n",
        "    except Exception as e:\n",
        "        log_pipeline_step(\"transformation\", f\"Merge failed: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Data Type Conversion\n",
        "    data[\"region\"].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "    # Validation: Check for data quality issues\n",
        "    missing_ids = data[\"id\"].isnull().sum()\n",
        "    if missing_ids > 0:\n",
        "        raise ValueError(f\"Transformation error: {missing_ids} missing IDs detected\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Loading\n",
        "def load_data(data, target_file=\"transformed_data.csv\"):\n",
        "    \"\"\"\n",
        "    Save the transformed data to a CSV file or a database.\n",
        "    \"\"\"\n",
        "    data.to_csv(target_file, index=False)\n",
        "    log_pipeline_step(\"loading\", f\"Data saved to {target_file}\")\n",
        "\n",
        "# Monitoring: Logging\n",
        "def log_pipeline_step(step, message):\n",
        "    \"\"\"\n",
        "    Logs pipeline progress.\n",
        "    \"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {step.upper()}: {message}\")\n",
        "\n",
        "# Main ETL Pipeline\n",
        "def etl_pipeline():\n",
        "    try:\n",
        "        log_pipeline_step(\"start\", \"ETL pipeline initiated\")\n",
        "\n",
        "        # Extraction\n",
        "        log_pipeline_step(\"extraction\", \"Extracting data\")\n",
        "        raw_data = extract_data()\n",
        "\n",
        "        # Transformation\n",
        "        log_pipeline_step(\"transformation\", \"Transforming data\")\n",
        "        transformed_data = transform_data(raw_data)\n",
        "\n",
        "        # Loading\n",
        "        log_pipeline_step(\"loading\", \"Loading data\")\n",
        "        load_data(transformed_data)\n",
        "\n",
        "        log_pipeline_step(\"complete\", \"ETL pipeline completed successfully\")\n",
        "    except Exception as e:\n",
        "        log_pipeline_step(\"error\", f\"Pipeline failed with error: {e}\")\n",
        "\n",
        "# Run the ETL pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    etl_pipeline()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-12-06 19:10:44] START: ETL pipeline initiated\n",
            "[2024-12-06 19:10:44] EXTRACTION: Extracting data\n",
            "[2024-12-06 19:10:44] EXTRACTION: CSV data extracted successfully\n",
            "[2024-12-06 19:10:44] EXTRACTION: API data extracted successfully\n",
            "[2024-12-06 19:10:46] EXTRACTION: Database data extracted successfully\n",
            "[2024-12-06 19:10:46] TRANSFORMATION: Transforming data\n",
            "[2024-12-06 19:10:46] LOADING: Loading data\n",
            "[2024-12-06 19:10:46] LOADING: Data saved to transformed_data.csv\n",
            "[2024-12-06 19:10:46] COMPLETE: ETL pipeline completed successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-aea5eb9a63ce>:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data[\"region\"].fillna(\"Unknown\", inplace=True)\n"
          ]
        }
      ]
    }
  ]
}