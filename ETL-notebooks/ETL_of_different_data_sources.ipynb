{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7dd75cf",
      "metadata": {
        "id": "f7dd75cf"
      },
      "source": [
        "\n",
        "# Data Extraction and Transformation\n",
        "\n",
        "This notebook demonstrates data extraction, transformation, and loading (ETL) processes using various sources:\n",
        "- PostgreSQL databases\n",
        "- Web scraping\n",
        "- IoT device APIs (e.g., ThingSpeak)\n",
        "- REST APIs\n",
        "\n",
        "Each section contains:\n",
        "1. **Data Extraction:** Code to fetch data from the respective source.\n",
        "2. **Data Transformation:** Cleaning and structuring the data.\n",
        "3. **Exporting Data:** Saving the transformed data to a CSV file for further analysis.\n",
        "\n",
        "### Prerequisites\n",
        "Install the required libraries before running the notebook:\n",
        "```bash\n",
        "!pip install psycopg2-binary requests beautifulsoup4 pandas\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95507383",
      "metadata": {
        "id": "95507383"
      },
      "source": [
        "\n",
        "## Example 1: Extracting Data from PostgreSQL\n",
        "\n",
        "We connect to a public PostgreSQL server, retrieve data, transform it, and save it to a CSV file.\n",
        "\n",
        "**Database Details:**\n",
        "- Host: `rajje.db.elephantsql.com`\n",
        "- Database: `ljayqbrj`\n",
        "- User: `ljayqbrj`\n",
        "- Password: `<your_password>`\n",
        "\n",
        "Replace `<your_password>` with your actual password for the database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f9d498",
      "metadata": {
        "id": "c9f9d498",
        "outputId": "c0cdc29a-65fb-4bf5-c578-c975095fa6ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to the PostgreSQL database!\n",
            "Data Extracted:\n",
            "PostgreSQL connection is closed.\n",
            "Transformed Data:\n",
            "    id                                        description  avg_length  \\\n",
            "30  50  provides comprehensive genomic view of plant l...      6659.0   \n",
            "45   7  is a database providing comprehensive annotati...      3086.0   \n",
            "49  28  a collaborative effort between leading researc...      2340.0   \n",
            "46  53  is a database of experimentally validated long...      2311.0   \n",
            "11  48  provides computational access to molecular-int...      2251.0   \n",
            "\n",
            "   length_category  num_sequences  num_organisms  \n",
            "30            Long         936926             80  \n",
            "45            Long             62             10  \n",
            "49            Long          11124              1  \n",
            "46            Long            933              3  \n",
            "11            Long             95              8  \n",
            "Transformed data written to /content/transformed_data.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-e70f6914d7ee>:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
            "  data_frame = pd.read_sql_query(query, connection)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import psycopg2\n",
        "\n",
        "# PostgreSQL connection details\n",
        "host = \"hh-pgsql-public.ebi.ac.uk\"\n",
        "port = 5432\n",
        "database = \"pfmegrnargs\"\n",
        "user = \"reader\"\n",
        "password = \"NWDMCE5xdipIjRrp\"\n",
        "\n",
        "try:\n",
        "    # Connect to PostgreSQL\n",
        "    connection = psycopg2.connect(\n",
        "        host=host,\n",
        "        port=port,\n",
        "        database=database,\n",
        "        user=user,\n",
        "        password=password\n",
        "    )\n",
        "    print(\"Connected to the PostgreSQL database!\")\n",
        "\n",
        "    # Query to fetch data (use the correct columns based on schema inspection)\n",
        "    query = \"\"\"\n",
        "    SELECT id, description, avg_length, min_length, max_length, num_sequences, num_organisms\n",
        "    FROM rnc_database\n",
        "    LIMIT 100;\n",
        "    \"\"\"\n",
        "    data_frame = pd.read_sql_query(query, connection)\n",
        "    print(\"Data Extracted:\")\n",
        "    data_frame.head()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "finally:\n",
        "    if connection:\n",
        "        connection.close()\n",
        "        print(\"PostgreSQL connection is closed.\")\n",
        "\n",
        "# Data Transformation\n",
        "try:\n",
        "    # 1. Add a new column 'length_category' based on the average length of sequences\n",
        "    data_frame['length_category'] = data_frame['avg_length'].apply(lambda x: 'Short' if x < 200 else 'Long')\n",
        "\n",
        "    # 2. Filter data: Retain only entries where the average length is greater than 150\n",
        "    filtered_data = data_frame[data_frame['avg_length'] > 150]\n",
        "\n",
        "    # 3. Sort data: Sort by average length in descending order\n",
        "    sorted_data = filtered_data.sort_values(by='avg_length', ascending=False)\n",
        "\n",
        "    # 4. Select relevant columns for export\n",
        "    transformed_data = sorted_data[['id', 'description', 'avg_length', 'length_category', 'num_sequences', 'num_organisms']]\n",
        "\n",
        "    # Display the transformed data\n",
        "    print(\"Transformed Data:\")\n",
        "    print(transformed_data.head())\n",
        "\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: {e}. Ensure the column names match the table structure.\")\n",
        "\n",
        "# Save the transformed data to a CSV file\n",
        "csv_file_path = \"/content/transformed_data.csv\"\n",
        "transformed_data.to_csv(csv_file_path, index=False)\n",
        "print(f\"Transformed data written to {csv_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15c8b674",
      "metadata": {
        "id": "15c8b674"
      },
      "source": [
        "\n",
        "## Example 2: Extracting Data from Web Pages (Web Scraping)\n",
        "\n",
        "We scrape quotes and their authors from the [Quotes to Scrape](http://quotes.toscrape.com) website.\n",
        "\n",
        "**Libraries Used:** `requests`, `BeautifulSoup`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cedf016",
      "metadata": {
        "id": "2cedf016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7db9d6-a91f-48cb-c7a4-e2b96e0a0e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Extracted:\n",
            "                                               quote           author\n",
            "0  “The world as we have created it is a process ...  Albert Einstein\n",
            "1  “It is our choices, Harry, that show what we t...     J.K. Rowling\n",
            "2  “There are only two ways to live your life. On...  Albert Einstein\n",
            "3  “The person, be it gentleman or lady, who has ...      Jane Austen\n",
            "4  “Imperfection is beauty, madness is genius and...   Marilyn Monroe\n",
            "Extracted data written to quotes_data.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Web scraping URL for quotes\n",
        "url = \"http://quotes.toscrape.com\"\n",
        "\n",
        "# Send GET request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the page content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract quotes and authors\n",
        "    quotes = soup.find_all('span', class_='text')\n",
        "    authors = soup.find_all('small', class_='author')\n",
        "\n",
        "    # Create a DataFrame\n",
        "    data = {\n",
        "        'quote': [quote.text for quote in quotes],\n",
        "        'author': [author.text for author in authors]\n",
        "    }\n",
        "    data_frame = pd.DataFrame(data)\n",
        "    print(\"Data Extracted:\")\n",
        "    print(data_frame.head())\n",
        "\n",
        "    # Save the extracted data to CSV\n",
        "    csv_file_path = 'quotes_data.csv'\n",
        "    data_frame.to_csv(csv_file_path, index=False)\n",
        "    print(f'Extracted data written to {csv_file_path}')\n",
        "else:\n",
        "    print(f\"Failed to fetch data from website. Status code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e49fc0a9",
      "metadata": {
        "id": "e49fc0a9"
      },
      "source": [
        "\n",
        "## Example 3: Extracting Data from IoT Devices (Public APIs)\n",
        "\n",
        "We retrieve data from a public ThingSpeak IoT channel and analyze temperature readings.\n",
        "\n",
        "**API URL:** `https://api.thingspeak.com/channels/12397/feeds.json?results=10`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13092e24",
      "metadata": {
        "id": "13092e24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380b1354-c982-4cba-9541-9a2c02083963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Extracted:\n",
            "             created_at  entry_id field1 field2 field3 field4 field5 field6  \\\n",
            "0  2024-12-05T08:36:46Z   5229127    270      0     95   32.6      0  29.39   \n",
            "1  2024-12-05T08:37:46Z   5229128    270      0     95   32.6      0  29.39   \n",
            "2  2024-12-05T08:38:46Z   5229129    270      0     95   32.5      0  29.39   \n",
            "3  2024-12-05T08:39:46Z   5229130    270      0     95   32.5      0  29.39   \n",
            "4  2024-12-05T08:40:46Z   5229131    270      0     95   32.6      0  29.39   \n",
            "\n",
            "  field7 field8  \n",
            "0  4.069      0  \n",
            "1  4.068      0  \n",
            "2  4.064      0  \n",
            "3  4.065      0  \n",
            "4  4.061      0  \n",
            "Transformed Data:\n",
            "             created_at  field1 temp_status\n",
            "0  2024-12-05T08:36:46Z     270  Above 25°C\n",
            "1  2024-12-05T08:37:46Z     270  Above 25°C\n",
            "2  2024-12-05T08:38:46Z     270  Above 25°C\n",
            "3  2024-12-05T08:39:46Z     270  Above 25°C\n",
            "4  2024-12-05T08:40:46Z     270  Above 25°C\n",
            "IoT device data written to iot_transformed_data.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Updated ThingSpeak API URL with a valid public channel\n",
        "api_url = \"https://api.thingspeak.com/channels/12397/feeds.json?results=10\"  # Example channel with temperature data\n",
        "\n",
        "# Send GET request to the ThingSpeak API\n",
        "response = requests.get(api_url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Load the JSON data from the response\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract the feed data (the sensor data from the API response)\n",
        "    feeds = data['feeds']\n",
        "\n",
        "    # Convert the feed data into a Pandas DataFrame\n",
        "    data_frame = pd.DataFrame(feeds)\n",
        "    print(\"Data Extracted:\")\n",
        "    print(data_frame.head())\n",
        "\n",
        "    # Data Transformation: Example transformations\n",
        "    try:\n",
        "        # Convert field1 to numeric (temperature example)\n",
        "        data_frame['field1'] = pd.to_numeric(data_frame['field1'], errors='coerce')\n",
        "\n",
        "        # 1. Add a new column 'temp_status' indicating if temperature is above or below 25°C\n",
        "        data_frame['temp_status'] = data_frame['field1'].apply(lambda x: 'Above 25°C' if x > 25 else 'Below 25°C')\n",
        "\n",
        "        # 2. Filter data: Retain only rows where the temperature is above 25°C\n",
        "        filtered_data = data_frame[data_frame['field1'] > 25]\n",
        "\n",
        "        # 3. Sort data: Sort by the temperature (field1) in descending order\n",
        "        sorted_data = filtered_data.sort_values(by='field1', ascending=False)\n",
        "\n",
        "        # 4. Select relevant columns for export\n",
        "        transformed_data = sorted_data[['created_at', 'field1', 'temp_status']]  # Adjust columns based on your IoT data\n",
        "        print(\"Transformed Data:\")\n",
        "        print(transformed_data.head())  # Display transformed data\n",
        "\n",
        "        # Save the transformed data to CSV\n",
        "        csv_file_path = 'iot_transformed_data.csv'\n",
        "        transformed_data.to_csv(csv_file_path, index=False)\n",
        "        print(f'IoT device data written to {csv_file_path}')\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"KeyError: {e}. Ensure the column names match the API structure.\")\n",
        "else:\n",
        "    print(f\"Failed to fetch data from ThingSpeak. Status code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3c8f7d",
      "metadata": {
        "id": "1c3c8f7d"
      },
      "source": [
        "\n",
        "## Example 4: Extracting Data from REST APIs\n",
        "\n",
        "We fetch data from a sample REST API, perform transformations, and save the results to a CSV file.\n",
        "\n",
        "**API URL:** `https://jsonplaceholder.typicode.com/posts`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "616ef16f",
      "metadata": {
        "id": "616ef16f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cbb2d1-684d-4a1f-9541-b3e8a0c737ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Extracted:\n",
            "   id              name   username                      email  \\\n",
            "0   1     Leanne Graham       Bret          Sincere@april.biz   \n",
            "1   2      Ervin Howell  Antonette          Shanna@melissa.tv   \n",
            "2   3  Clementine Bauch   Samantha         Nathan@yesenia.net   \n",
            "3   4  Patricia Lebsack   Karianne  Julianne.OConner@kory.org   \n",
            "4   5  Chelsey Dietrich     Kamren   Lucio_Hettinger@annie.ca   \n",
            "\n",
            "                                             address                  phone  \\\n",
            "0  {'street': 'Kulas Light', 'suite': 'Apt. 556',...  1-770-736-8031 x56442   \n",
            "1  {'street': 'Victor Plains', 'suite': 'Suite 87...    010-692-6593 x09125   \n",
            "2  {'street': 'Douglas Extension', 'suite': 'Suit...         1-463-123-4447   \n",
            "3  {'street': 'Hoeger Mall', 'suite': 'Apt. 692',...      493-170-9623 x156   \n",
            "4  {'street': 'Skiles Walks', 'suite': 'Suite 351...          (254)954-1289   \n",
            "\n",
            "         website                                            company  \n",
            "0  hildegard.org  {'name': 'Romaguera-Crona', 'catchPhrase': 'Mu...  \n",
            "1  anastasia.net  {'name': 'Deckow-Crist', 'catchPhrase': 'Proac...  \n",
            "2    ramiro.info  {'name': 'Romaguera-Jacobson', 'catchPhrase': ...  \n",
            "3       kale.biz  {'name': 'Robel-Corkery', 'catchPhrase': 'Mult...  \n",
            "4   demarco.info  {'name': 'Keebler LLC', 'catchPhrase': 'User-c...  \n",
            "Transformed Data:\n",
            "   id                        full_name                   email  \\\n",
            "9  10  Clementina DuBuque (Hoeger LLC)  Rey.Padberg@karina.biz   \n",
            "\n",
            "                                             address         phone  \n",
            "9  {'street': 'Kattie Turnpike', 'suite': 'Suite ...  024-648-3804  \n",
            "Transformed data written to transformed_api_data.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# API URL for JSONPlaceholder\n",
        "api_url = \"https://jsonplaceholder.typicode.com/users\"\n",
        "\n",
        "# Send a GET request to the API to fetch data\n",
        "response = requests.get(api_url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Load the JSON data from the response\n",
        "    data = response.json()\n",
        "\n",
        "    # Convert the data into a Pandas DataFrame\n",
        "    data_frame = pd.DataFrame(data)\n",
        "    print(\"Data Extracted:\")\n",
        "    print(data_frame.head())  # Display the first few rows of the extracted data\n",
        "\n",
        "    # Data Transformation: Example transformations\n",
        "    try:\n",
        "        # 1. Example transformation - Create a new column 'full_name' by combining 'name' and 'company'\n",
        "        data_frame['full_name'] = data_frame['name'] + \" (\" + data_frame['company'].apply(lambda x: x['name']) + \")\"\n",
        "\n",
        "        # 2. Filter data: Retain only entries where the city of the user is 'Lebsackbury'\n",
        "        filtered_data = data_frame[data_frame['address'].apply(lambda x: x['city'] == 'Lebsackbury')]\n",
        "\n",
        "        # 3. Sort data: Sort by the user's name alphabetically\n",
        "        sorted_data = filtered_data.sort_values(by='name')\n",
        "\n",
        "        # 4. Select relevant columns for export\n",
        "        transformed_data = sorted_data[['id', 'full_name', 'email', 'address', 'phone']]  # Adjust columns based on your needs\n",
        "        print(\"Transformed Data:\")\n",
        "        print(transformed_data.head())  # Display transformed data\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"KeyError: {e}. Ensure the column names match the API structure.\")\n",
        "\n",
        "    # Save the transformed data to a CSV file\n",
        "    csv_file_path = 'transformed_api_data.csv'\n",
        "    transformed_data.to_csv(csv_file_path, index=False)\n",
        "    print(f'Transformed data written to {csv_file_path}')\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to fetch data from API. Status code: {response.status_code}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}